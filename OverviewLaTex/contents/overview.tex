\section{Отчет}
\begin{itemize}
    \item \textbf{Общая мотивация} \\
    Рассматривается выборка, сгенерированная с помощью \href{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html}{\text{make\_circles}} - 2 вложенных круга с небольшим шумом. \\ Из здравого смысла несложно видеть разделяющее пространство - круг между нашими двумя. Тем не менее, обычные линейные методы очевидно затрудняются построить хорошее приближение. \\
    Добавим всего один признак, уже визуально отчетливо видно, что выборка в новом пространстве легко разделяется гиперплоскостью - то, чего мы и добивались. Оказывается, эту идею перехода к новому пространству признаков можно обобщить и на менее тривиальные ситуации.
     \item \textbf{Анализ результатов SVM с различными ядрами} \\
         Рассматривается выборка \href{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html}{ijcnn1}. Этот датасет использовался в рамках конкурса нейросетей \textit{IJCNN}. \\
         Данные были разбиты случайно на трейн и тест в соотношении 7:3. \\
         Полученные результаты следующие - на тестовой выборке размера $\approx 35000$ примерно в 2 раза быстрее обучались SVM с ядрами $poly, \ rbf$, и показали заметно лучший перформанс относительно $linear$: \\ увеличение $ROC-AUC$ метрики на $\approx 0.08$ и $Precision-Recall-Area$ на $\approx 0.3$. \\
         $Sigmoid$ ядро провалилось по всем статьям - долго обучалось и показало наихудший результат, что свидетельствует о том, что этот метод наиболее чувствителен к исходной задаче и подбору гиперпараметров.
      \item \textbf{Приближение ядер с помощью RFF} \\  
         Рассматривается выборка \href{https://www.tensorflow.org/datasets/catalog/fashion_mnist}{fashion\_mnist} из пакета Tensorflow. \\
         Так как прямое использование ядерных функций может быть затратно по времени и памяти, иногда помогает метод аппроксимации ядер через построение случайных признаков на основе исходных. \\
         Реализованное приближение гауссовского ядра через RFF показало хороший результат - обучается быстрее, чем библиотечный подсчет напрямую, при этом не проседая в эффективности. Мы явно воспользовались тем, что для гауссовых ядер легко построить распределение, позволяющее применить RFF напрямую. \\
         Снова увидели неустойчивость $sigmoid$ ядра, которое и здесь оказалось неудачным выбором. \\
         
         
         \begin{tabular}{ |p{5cm}|p{5cm}|  }
        \hline
        \multicolumn{2}{|c|}{Результаты на тестовой выборке} \\
        \hline
        Метод & Accuracy\\
        \hline
        SVM, gaussian kernel,  RFF   & 0.8715\\
        LogReg, gaussian kernel,  RFF   & 0.8574\\
        SVM, linear kernel   & 0.8464\\
        SVM, polynomial kernel    & 0.863\\
        SVM, rbf kernel    & 0.8828\\
        SVM, sigmoid kernel    & 0.4321\\
        \hline
        \end{tabular}
\end{itemize}