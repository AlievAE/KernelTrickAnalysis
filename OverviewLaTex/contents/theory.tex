
\section{Пререквизиты}
Введем основные понятия, которыми далее будем оперировать. \
\begin{enumerate}
  \item \textbf{Постановка задачи} \
  
        На вход подаются данные в формате $X \in \mathcal{R}^{n \times d}$, где n - размер выборки, d - размерность пространства признаков и $y \in \mathcal{R}^n$ - таргетный признак. 
        Тогда наша задача - построить модель, которая будет предсказывать таргетный признак по входу из вектора размера d. В случае, когда таргетный признак является категориальным, будем говорить, что это задача классификации, если же класса всего 2 - то бинарной классификации.
    \item \textbf{Используемые методы} \
        \begin{itemize}
            \item Линейная регрессия \
            
                Предполагаем, что зависимость таргета от признаков линейная и вводим функцию штрафа: \
                $Q(\theta) = \norm{X\theta - y}^2 + \lambda G(\theta) \rightarrow \min$, где $G(\theta)$ - регуляризатор. \
                Если $G(\theta) \equiv \norm{\theta}^2$, то будем говорить о Ridge-регресии.
                
            \item Метод опорных векторов(SVM) \
            
                Хотим разделить выборку гиперплоскостью, но понимаем, что это скорее всего невозможно. \ 
                Определим задачу следующим образом: \ 
                $\begin{cases} \dfrac{1}{2}\norm{\theta}^2 + C \sum\limits_{i=1}^d \epsilon_i \rightarrow \min \limits_{\theta, b, \epsilon} \\ 0 \leq \epsilon_i \\ y_i(\langle \theta, x_i \rangle + b) \geq 1 - \epsilon_i \end{cases}$. \
                
                Здесь $\dfrac{2}{\norm{\theta}}$ - ширина разделяющей полосы, $\dfrac{1}{\norm{\theta}}$ - мягкий отступ, $\epsilon$ - штраф за попадание внутрь разделяющей полосы. Чем больше $C$, тем сильнее мы ориентируемся на обучающую выборку.
                
                Эта задача является выпуклой и имеет единственное решение, а еще она удовлетворяет условиям Каруша-Куна-Таккера, чем мы не постеснялись воспользоваться.
            \item Ядра \
            
                Ядро - скалярное произведение в некотором пространстве. \
                Согласно теореме Мерсера, функция $K(x, z)$ является ядром тогда и только тогда, когда она симметрична и неотрицательно определена. \ 
                Проверять эти условия на практике зачастую сложно, поэтому обычно пользуются конструктивными признаками: 
                
                \begin{theorem}
                \label{th:kernelConstr}
                    Пусть~$K_1(x, z)$ и~$K_2(x, z)$~--- ядра, заданные на множестве~$X$,
                    $f(x)$ -  вещественная функция на~$X$,
                    $\phi: X \to \R^N$ -  векторная функция на~$X$,
                    $K_3$ - ядро, заданное на~$\R^N$.
                    Тогда следующие функции являются ядрами:
                    \begin{enumerate}
                        \item $K(x, z) = K_1(x, z) + K_2(x, z)$,
                        \item $K(x, z) = \alpha K_1(x, z)$, $\alpha > 0$,
                        \item $K(x, z) = K_1(x, z) K_2(x, z)$,
                        \item $K(x, z) = f(x) f(z)$,
                        \item $K(x, z) = K_3(\phi(x), \phi(z))$.
                    \end{enumerate}
                \end{theorem} \
                 \
                
            \item Основные типы ядер
            \begin{enumerate}
                \item Линейное \ 
                
                    $K(x, z) = \langle x, z \rangle$ \\
                    Базовое ядро
                \item Полиномиальное \
                
                $K(x, z) = (\langle x, z \rangle + R)^m$ \\
                Соответсвует переводу набора признаков в мономы степени не более $m$, $R$ регулирует вес признаков.
                \item Radial Basis Function(RBF) или Гауссово \
                    
                $K(x, z) = exp(- \dfrac{\norm{x-z}^2}{2\sigma^2})$ \
                \item Sigmoid \
                
                $K(x, z) = \tanh(a\langle x, z \rangle + R)$
            \end{enumerate}
            
            \item Random Fourier Features \\
        Из комплексного анализа известно, что любое непрерывное ядро вида~$K(x, z) = K(x - z)$
является преобразованием Фурье некоторого вероятностного распределения~(теорема Бохнера):
\[
    K(x - z)
    =
    \int_{\RR^d}
        p(\theta)
        e^{i \theta^T (x - z)}
    d\theta.
\]
Преобразуем интеграл:
\begin{align*}
    \int_{\RR^d}
        p(\theta)
        e^{i \theta^T (x - z)}
    d\theta
    &=
    \int_{\RR^d}
        p(\theta)
        \cos(\theta^T (x - z))
    d\theta
    +
    i
    \int_{\RR^d}
        p(\theta)
        \sin(\theta^T (x - z))
    d\theta
    =\\
    &=
    \int_{\RR^d}
        p(\theta)
        \cos(\theta^T (x - z))
    dw.
\end{align*}
Поскольку значение ядра~$K(x - z)$ всегда вещественное,
то и в правой части мнимая часть равна нулю,
а значит, остаётся лишь интеграл от косинуса~$\cos \theta^T (x - z)$.
Мы можем приблизить данный интеграл методом Монте-Карло:
\[
    \int_{\RR^d}
        p(\theta)
        \cos(\theta^T (x - z))
    d\theta
    \approx
    \frac{1}{n}
    \sum_{j = 1}^{n}
        \cos(\theta_j^T (x - z)),
\]
где векторы~$\theta_1, \dots, \theta_n$ генерируются из распределения~$p(\theta)$.
Используя эти векторы, мы можем сформировать аппроксимацию преобразования~$\phi(x)$:
\[
    \tilde \phi(x)
    =
    \frac{1}{\sqrt{n}}
    (\cos(\theta_1^T x), \dots, \cos(\theta_n^T x),
    \sin(\theta_1^T x), \dots, \sin(\theta_n^T x)).
\]
Данная оценка является несмещённой для~$K(x, z)$ в силу свойств метода Монте-Карло.
Более того, с помощью неравенств концентрации меры можно показать, что дисперсия данной оценки достаточно низкая.
Заметим, что синусы можно заменить на косинусы со сдвигом, что немного упрощает реализацию метода.
        \end{itemize}
        
        
\end{enumerate}